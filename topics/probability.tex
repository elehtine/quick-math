\begin{topic}{Probability}
  Probability space is $(\Omega, \F, \prob)$.
  Probability function is \emph{any} function $\prob : \F \rightarrow \R$
  that satisfies
  \begin{enumerate}
    \item $\forall E, 0 \leq \prob(E) \leq 1$
    \item $\prob(\Omega) = 1$
    \item $\prob \left( \bigcup\limits_{i \geq 1} E_i \right)
      = \sum\limits_{i \geq 1} \prob(E_i)$
  \end{enumerate}
  Events E and F are independent if and only if
  \[
    \prob(E \cap F) = \prob(E) \cdot \prob(F)
  \]
  Conditional probability
  \[
    \prob(E \given F) = \frac{\prob(E \cap F)}{\prob(F)}
  \]
  Law of Total Probability
  \[
    \prob(B)
    = \summation \prob(B \cap E_i)
    = \summation \prob(B \given E_i) \prob(E_i)
  \]
  Bayes' Law
  \[
    \prob(E_i \given B)
    = \frac{\prob(E_i \cap B)}{\prob(B)}
    = \frac{\prob(B \given E_i) \prob(E_i)}
    {\summation[j = 1] \prob(B \given E_j) \prob(E_j)}
  \]
  Linearity of Expectations
  \[
    \E \left[ \sum_{i = 1}^n X_i \right]
    = \sum_{i = 1}^n E[X_i]
  \]
  Jensen's Inequality. If $f$ is a convex function, then
  \[
    \E[ f(X) ] \geq f( \E[X] )
  \]
\end{topic}
